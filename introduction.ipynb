{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Quick Introduction to Automated Machine Learning (AutoML) and the \"Mankind Vs Machine\" Community Challenge\n",
    "\n",
    "Geoff Pidcock | 20181103 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Section0'></a>\n",
    "## Motivation\n",
    "The topic of Automated Machine Learning (AutoML) has been buzzing throughout 2018, with big players like Google [making it a focus of their keynotes](https://techcrunch.com/2018/01/17/googles-automl-lets-you-train-custom-machine-learning-models-without-having-to-code/), and automation platform providers like [DataRobot securing crazy amounts of funding](https://www.businesswire.com/news/home/20181025005739/en/DataRobot-Raises-100-Million-Series-Led-Meritech) in order to scale up. <br>\n",
    "\n",
    "As a data analyst responsible for delivering predictive analytic products to an Australian Business, I decided to do some research into the topic. I wanted to find out:\n",
    "- [What, exactly, is AutoML](#Section2)\n",
    "- [What the tools can and can't do - and what value they could bring to my trade](#Section3)\n",
    "- [How to get started using these tools (with some code)](#Section4)\n",
    "- [How good are these tools really (and should I be worried about being out of a job... :D) ](#Section5)\n",
    "- [Whether the community was also interested in exploring these tools](#Section1)\n",
    "- [Where to learn more, and follow developments in AutoML](#Section6)\n",
    "\n",
    "Here's what I've found out. \n",
    "\n",
    "**IMPORTANT NOTE:** <br> \n",
    "If you're considering competing in the [\"Mankind Vs Machine\" Machine Learning Challenge starting Friday Nov 9](https://ga.co/2PtgkLn), and haven't used Kaggle Before, [I would encourage skipping through to the code](#Section4), and attempting a Kaggle submission on the [Titanic Challenge](https://www.kaggle.com/c/titanic) before the event. \n",
    "\n",
    "[You can learn more about this challenge here.](#Section1)\n",
    "\n",
    "#### Figure 1: Interest over time, for a number of keywords relating to AutoML tools/platforms\n",
    "<img src=\"./images/interest_over_time.png\" style=\"width:450px;height:321px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Section2'></a>\n",
    "## What, exactly, is AutoML?\n",
    "[Go back to top](#Section0) <br>\n",
    "In a nut shell: AutoML is **Machine Learning for Machine Learning**! Simple :D <br>\n",
    "<img src=\"./images/inception.jpeg\" style=\"width:512px;height:256px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quoting [Sibanjan Das and Umit Mert Cakmak](https://www.packtpub.com/big-data-and-business-intelligence/hands-automated-machine-learning),\n",
    "\n",
    ">AutoML aims to **ease the process of building ML models by automating commonly-used steps**, such as feature preprocessing, model selection, and hyperparameters tuning.\n",
    "\n",
    "AutoML isn't any one particular tool or platform ([as argued very eloquently by Rachel Thomas](https://www.fast.ai/2018/07/23/auto-ml-3/))- it is a field of machine learning including research, open-source AutoML libraries, workshops, and competitions (including a [community challenge coming up in Sydney next week](http://bit.ly/2CNsaJN)). <br>\n",
    "\n",
    "This field has produced a number of tools that can be used in your data science projects, including: <br>\n",
    "\n",
    "*Free* <br>\n",
    "- [TEAPOT (Python)](https://github.com/EpistasisLab/tpot)\n",
    "- [Auto-SKLearn (Python)](https://automl.github.io/auto-sklearn/stable/)  -warning, not Windows compatible!\n",
    "- [MLBox (Python)](https://github.com/AxeldeRomblay/MLBox)\n",
    "- [Featuretools (Python)](https://www.featuretools.com/)\n",
    "\n",
    "*Open with Paid Support* <br>\n",
    "- [H2O.AI (Various)](http://docs.h2o.ai/)\n",
    "\n",
    "*Provided at Cost* <br>\n",
    "- [DataRobot (Various Languages, GUI)](https://www.datarobot.com/product/)\n",
    "- [Google Cloud AutoML (Various Languages)](https://cloud.google.com/automl/)\n",
    "- [Einstein AI (Various Languages, GUI)](https://developer.salesforce.com/einstein/)\n",
    "<br>\n",
    "\n",
    "**Note**: There is an absence of R specifric tools in this list! I'd love to hear from R coders who know of, or could recommend, AutoML packages.\n",
    "\n",
    "These tools have a wide range of capabilities. The next section contains a short dive into what they can and can't do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Section3'></a>\n",
    "## What the tools can and can't do - and what value they could bring to your trade\n",
    "[Go back to contents](#Section0) <br>\n",
    "Creating a predictive analytics product is a messy process, involving many steps. A popular model for this process is [CRISP-DM](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining):\n",
    "\n",
    "#### Figure 2: Overview of a predictive analytics project process (CRISP-DM)\n",
    "*Highlighting is added to the stages where AutoML typically provides value.*\n",
    "<img src=\"./images/crisp-dm.png\" style=\"width:603px;height:421px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of steps are presently out of scope of the tools mentioned above, including: \n",
    "- \"Business Understanding\" - automated services do not presently replace the need for consultation and domain expertise.\n",
    "- **\"Data Understanding\"** - automated services still do not manage data collection/description/tagging. No matter how much I wish it was otherwise, in 2018, it still is \"garbage in, garbage out\"!\n",
    "- \"Deployment\" - though the tools can assist deployment (e.g. by wrapping everything up with a nice API), there is still all that project management and system implementation goodness that one cannot do away with, yet.\n",
    "\n",
    "So in my opinion, **what value could they bring to your trade?**\n",
    "1. **Reduce the burden of DataPrep.** <br>Based upon the [Kaggle State of ML Survey 2017](https://www.kaggle.com/surveys/2017), the majority of a data scientist's time is spent on data understanding and data preparation. Though there is no silver bullet for data understanding, having a system manage the model specific needs of your data (like nulls, encoding, or derived features aka feature engineering) could be a major win. Competitions like the [AutoML ChaLearn](http://automl.chalearn.org/) have demonstrated these capabilities in tools like Auto-SKLearn.\n",
    "2. **Establish a new benchmark to beat by bespoke data prep/modelling.** <br> This seems to be the opinion of most experienced data scientists I speak to.\n",
    "3. **Provide a 'good enough' model so that you can focus on the other time consuming, non negotiable, parts of the project.** <br> I don't imagine this will be a popular opinion - as data science training and competitions tend to emphasize the importance of  model selection and hand-tuned hyper-parameters. Those that work on their own (i.e. in a startup) or in small/dislocated teams may agree though, as **the value is not in the model, but in the business outcome**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Section4'></a>\n",
    "## How to get started using these tools (with some code)\n",
    "[Go back to top](#Section0) <br>\n",
    "Using an Auto-ML tool is a matter of \n",
    "- installing it, \n",
    "- understanding the AutoML data prep requirements, \n",
    "- instantiating AutoML as an estimator,\n",
    "- and giving the tool plenty of time to train - as *AutoML unfortunately does not mean quick ML!*\n",
    "\n",
    "The following section demonstrates how to use the AutoML library TPOT in generating a survival prediction on the famous Titanic Dataset. <br>\n",
    "Some of this code has been adapted from [Sibanjan Das and Umit Mert Cakmak](https://www.packtpub.com/big-data-and-business-intelligence/hands-automated-machine-learning), and from the [TPOT titanic tutorial](https://github.com/EpistasisLab/tpot/blob/master/tutorials/Titanic_Kaggle.ipynb). <br>\n",
    "The data has been [sourced from the Titanic Demonstration Kaggle Competition](https://www.kaggle.com/c/titanic/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPOT AutoML Example\n",
    "#### Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the AutoML library\n",
    "import sys\n",
    "!{sys.executable} -m pip install tpot\n",
    "# Note - there are optional extras needed to use other elements of TPOT - see the docs: http://epistasislab.github.io/tpot/installing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.dummy import DummyClassifier # baseline\n",
    "from sklearn.ensemble import RandomForestClassifier # Random forest estimator - will be used later for comparison\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Data Prep Requirements\n",
    "For TPOT to work, some basic data prep is needed. This includes\n",
    "- relabelling of the training data features (i.e. the target is relabelled to 'class')\n",
    "- management of null values\n",
    "- categorical feature encoding\n",
    "- removal of identifiers or other irrelevant features\n",
    "\n",
    "Note: Each library (or platform) will have it's own requirements, so it's important to read the docs. <br> \n",
    "[You can find TPOT's docs here](https://epistasislab.github.io/tpot/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the kaggle train and test datasets\n",
    "titanic_train = pd.read_csv(\"./data/train.csv\")\n",
    "titanic_test = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Quickly inspecting training data\n",
    "titanic_train.info()\n",
    "\"\"\"\n",
    "Looks like we'll have to:\n",
    "- DROP Name, Ticket\n",
    "- ENCODE Sex, Embarked, potentially Cabin (though might be better to drop it given the nulls)\n",
    "- Fill nulls in Age, Embarked,\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep training data\n",
    "titanic_train_prepped = titanic_train.copy()\n",
    "\n",
    "# TPOT requires the tagged column to be renamed as \"Class\"\n",
    "titanic_train_prepped.rename(columns={'Survived': 'class'}, inplace=True)\n",
    "\n",
    "# encode categorical variables\n",
    "titanic_train_prepped['Sex'] = titanic_train_prepped['Sex'].map({'male':0,'female':1})\n",
    "titanic_train_prepped['Embarked'] = titanic_train_prepped['Embarked'].map({'S':0,'C':1,'Q':2})\n",
    "\n",
    "# fill nulls\n",
    "titanic_train_prepped = titanic_train_prepped.fillna(-999)\n",
    "\n",
    "# removal of identifiers and other features\n",
    "titanic_train_prepped = titanic_train_prepped.drop(['Name','Ticket','Cabin'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split titanic data into test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(titanic_train_prepped.drop(columns=['class']),\\\n",
    "                                                    titanic_train_prepped['class'],\\\n",
    "                                                    train_size=0.75,\\\n",
    "                                                    test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the same processing to the Kaggle test data\n",
    "titanic_test_prepped = titanic_test.copy()\n",
    "\n",
    "# encode categorical variables\n",
    "titanic_test_prepped['Sex'] = titanic_test_prepped['Sex'].map({'male':0,'female':1})\n",
    "titanic_test_prepped['Embarked'] = titanic_test_prepped['Embarked'].map({'S':0,'C':1,'Q':2})\n",
    "\n",
    "# fill nulls\n",
    "titanic_test_prepped = titanic_test_prepped.fillna(-999)\n",
    "\n",
    "# removal of identifiers and other features\n",
    "titanic_test_prepped = titanic_test_prepped.drop(['Name','Ticket','Cabin'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure test dataset is the same shape as train\n",
    "assert (titanic_train_prepped.drop(columns=['class']).shape[1] == titanic_test_prepped.shape[1]), \"Not Equal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiating AutoML the Estimator\n",
    "The code below uses some standard hyperparameter settings when instantiating TPOT. <br>\n",
    "An important TPOT parameter to set is the number of generations. Why? Because training TPOT with lots of generations can take a long time. \n",
    "[Quoting this reference:](https://github.com/EpistasisLab/tpot/blob/master/tutorials/Titanic_Kaggle.ipynb) <br>\n",
    "> On a standard laptop with 4GB RAM, it roughly takes 5 minutes per generation to run. For each added generation, it should take 5 mins more. Thus, for the default value of 100, total run time could be roughly around 8 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot = TPOTClassifier(generations=10, population_size=30, verbosity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ... And giving it plenty of time to train! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123e87c6d412475f8982be653897ae36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=330, style=ProgressStyle(description_width='initial')), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.8100381135259376\n",
      "Generation 2 - Current best internal CV score: 0.8100381135259376\n",
      "Generation 3 - Current best internal CV score: 0.8100381135259376\n",
      "Generation 4 - Current best internal CV score: 0.8175012988524379\n",
      "Generation 5 - Current best internal CV score: 0.8175012988524379\n",
      "Generation 6 - Current best internal CV score: 0.8175012988524379\n",
      "Generation 7 - Current best internal CV score: 0.8175123546843892\n",
      "Generation 8 - Current best internal CV score: 0.8190048919978221\n",
      "Generation 9 - Current best internal CV score: 0.8190048919978221\n",
      "Generation 10 - Current best internal CV score: 0.8279385029738526\n",
      "\n",
      "Best pipeline: LogisticRegression(RandomForestClassifier(input_matrix, bootstrap=False, criterion=gini, max_features=0.25, min_samples_leaf=10, min_samples_split=11, n_estimators=100), C=1.0, dual=False, penalty=l1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(config_dict=None, crossover_rate=0.1, cv=5,\n",
       "        disable_update_check=False, early_stop=None, generations=10,\n",
       "        max_eval_time_mins=5, max_time_mins=None, memory=None,\n",
       "        mutation_rate=0.9, n_jobs=1, offspring_size=None,\n",
       "        periodic_checkpoint_folder=None, population_size=30,\n",
       "        random_state=None, scoring=None, subsample=1.0, use_dask=False,\n",
       "        verbosity=2, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When you invoke fit method, TPOT will create generations of populations, \n",
    "# seeking best set of parameters. Arguments you have used to create\n",
    "# TPOTClassifier such as generations and population_size will affect the\n",
    "# search space and resulting pipeline.\n",
    "tpot.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Thankfully after it's done, you can export the pipeline summarized above. \n",
    "tpot.export('example_tpot_pipeline.py')\n",
    "# It's quite interesting to read the code it generates - take a look!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So how did the TPOT do?\n",
    "- Does it beat a dummy classifier?\n",
    "- Does it beat a Random Forest?\n",
    "- Does it rank well on Kaggle :p?\n",
    "\n",
    "Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.820627802690583"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting TPOT's accuracy score\n",
    "tpot.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5022421524663677"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does it beat the dummy?\n",
    "dummy = DummyClassifier()\n",
    "dummy.fit(X_train, y_train)\n",
    "dummy.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It beats the dummy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8071748878923767"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does it beat the Random Forest (my personal favourite estimator, and arguably it's own flavour of AutoML, given it handles ensembling)\n",
    "rfclf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1)\n",
    "rfclf.fit(X_train, y_train)\n",
    "rfclf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It beats the Random Forest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does it rank well on Kaggle?\n",
    "submission = tpot.predict(titanic_test_prepped)\n",
    "\n",
    "# Create the submission file\n",
    "final = pd.DataFrame({'PassengerId': titanic_test_prepped['PassengerId'], 'Survived': submission})\n",
    "final.to_csv('data/submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It doesn't rank well on Kaggle :p** <br>\n",
    "The score from this run-through was ~0.73.\n",
    "It did previously beat the common sense \"Gender Model\" of \"women survived the titanic\" <br>\n",
    "(i.e. if Sex = F, Survived =1)\n",
    "\n",
    "#### Figure 3: Kaggle rankings of slapdash TPOT model. \n",
    "*The ranking shown is the first submission. <br>\n",
    "The second run through did not beat the first submission, or the common sense benchmark (labelled \"Gender Based Model\"). *\n",
    "\n",
    "<img src=\"./images/kaggle_kinda_fail.png\" style=\"width:673px;height:500px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Section5'></a>\n",
    "## How good are these tools really? (and should I be worried about being out of a job... :D)\n",
    "[Go back to top](#Section0) <br>\n",
    "\n",
    "This is actually a really hard question to answer! <br>\n",
    "\n",
    "I can see the value of TPOT to my trade, in providing an easy new benchmark to beat, and providing some inspiration towards model selection and pipeline construction. It is certainly easier kicking off TPOT and leaving it to brew, than thinking through every possible model and searching through each hyper-parameter! <Br>\n",
    "\n",
    "**As for should I be worried about being out of a job...** <br>\n",
    "\n",
    "*On the side of **No, I shouldn't**:*\n",
    "- The rest of the Data Science and Predictive Analytics process is hard and technical, and out of the scope of the tools I've come across so far. (i.e. it's more a productivity tool than a replacement tool).\n",
    "- TPOT sucked at Kaggle - take that, computers!\n",
    "\n",
    "*On the side of **Yes, I should**:*\n",
    "- Some of those other steps in the Data Science process could be executed by good project or product manager, with a sound understanding of \"democratized\" AutoML tools. \n",
    "- TPOT's poor Kaggle performance was likely errors and missed opportunities on my part, including:\n",
    ">- Thoughtless data pre-processing \n",
    "(i.e. I could have imputed the age, rather than filled it with -999!)\n",
    ">- No Application of Domain Knowledge  \n",
    "(i.e. I could have introduced a feature \"child\" based on the value of age, and a model with clear \"women\" and \"child\" flags would likely perform much better than a model with sex and age).\n",
    "<br>\n",
    "- There are other tools to TPOT! Tools like Auto-SKLearn and Featuretools can help with more of the data preparation than TPOT. \n",
    "- The team behind AlphaGo are actively at work applying reinforcement learning to the problem of AutoML ([see this paper](https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxhdXRvbWwyMDE4aWNtbHxneDo0OTQzOThjNmZmYjYxYjc3)). <br> \n",
    "- **I can certainly imaging a future sometime soon of a Kaggler being upstaged by an automatic opponent, much like Lee Sedol at Go **\n",
    "\n",
    "So I dunno! <br>\n",
    "¯\\\\_(ツ)_/¯\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Section1'></a>\n",
    "## HELP US EXPLORE AUTO-ML AT THE MANKIND VS MACHINE COMMUNITY CHALLENGE!\n",
    "[Go back to top](#Section0) <br>\n",
    "\n",
    "Join over 40 other people in exploring the capabilities of AutoML, and in trying to beat their predictions :) <br>\n",
    "\n",
    "It's kicks off this Friday at GA Sydney! <br>\n",
    "[You can register for the kick off here:](https://ga.co/2PtgkLn) <br>\n",
    ">\"Given a dataset and a problem, can your prediction beat an AutoML service?\" <br>\n",
    ">The dataset and prediction problem will be made available to attendees during the event (on Friday morning 9th of November), and submissions will be managed using a Kaggle In Class competition. \n",
    ">The competition will close Wednesday the 21st of November, and a winner will be judged on Friday the 23rd of November. <br>\n",
    ">The judging criteria will depend on the dataset, but it is likely to be the submission with greatest prediction accuracy. <br>\n",
    ">The prize for winner is glory, a mix of GA swag, $$$, and GA class credit.\n",
    "\n",
    "Think of it as a free opportunity to get familiar with these tools, and learn about their strengths and weaknesses with the rest of the Sydney Data Science community.\n",
    "\n",
    "#### Figure 4: Come along and defeat the adorable robots!\n",
    "\n",
    "<img src=\"./images/Japanese-Technology-Robotic-Mall-Robot-Japan-1964072.jpg\" style=\"width:512px;height:356px;\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Section6'></a>\n",
    "## Where to learn more, and follow developments in AutoML?\n",
    "[Go back to top](#Section0) <br>\n",
    "\n",
    "*Learn More*\n",
    "- [BOOK: Das and Mert Cakmak; Hands on Machine Learning; April 2018; Packt Publishing](https://www.packtpub.com/big-data-and-business-intelligence/hands-automated-machine-learning)\n",
    "- [BOOK: Hutte, Kotthoff, Vanschoren et al; AUTOML: Methods, Systems, Challenges; 2018](https://www.automl.org/book/)\n",
    "\n",
    "*Follow Developments*\n",
    "- [The Freidburg AutoML Group - makers of Auto-SKLearn, and managers of the ChaLearn AutoML Challenge](https://twitter.com/AutoMLFreiburg)\n",
    "- [Paper submissions to the AutoML Track of ICML - e.g. 2018](https://sites.google.com/site/automl2018icml/accepted-papers)\n",
    "\n",
    "If I've missed anything, let me know, and I'll update this list!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
